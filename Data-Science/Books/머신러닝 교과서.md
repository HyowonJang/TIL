## 1장 컴퓨터는 데이터에서 배운다

### 1.1 데이터를 지식으로 바꾸는 지능적인 시스템 구축

- 머신 러닝은 20세기 후반 데이터에서 지식을 추출하여 예측하는 자기 학습 알고리즘과 관련된 인공 지능의 하위분야로 출현했다.

### 1.2 머신 러닝의 세 가지 종류

- 지도 학습 : 레이블된 데이터, 직접 피드백, 출력 및 예측
- 비지도 학습 : 레이블 및 타깃 없음, 피드백 없음, 데이터에서 숨겨진 구조 찾기
- 강화 학습 : 결정 과정, 보상 시스템, 연속된 행동에서 학습

#### 1.2.1 지도 학습으로 미래 예측

- 분류 : 지도 학습 알고리즘을 사용하여 클래스를 `구분`할 수 있는 규칙을 학습한다. 이 규칙은 결정 경계가 된다.
  ![결정경계](image/결정경계.jpg)
- 회귀 : 독립변수와 종속변수가 주어졌을 때 출력값을 예측하는 두 변수 사이의 `관계`를 찾는다. 입력 x와 타깃 y가 주어지면 샘플과 거리가 최소가 되는 직선을 긋는다.
  ![선형회귀](image/선형회귀.jpg)

#### 1.2.2 강화 학습으로 반응형 문제 해결

- 강화 학습은 환경(체스판의 상태 등)과 상호 작용하여 시스템 성능을 향상하는 것이 목적이다. 강화 학습의 피드백은 정답 레이블이나 값이 아닌, 보상 함수로 얼마나 행동이 좋은지를 측정한 값이다. 강화 학습은 피드백(체스에서 이전 차례로 말미암아 어떤-위협적인 혹은 긍정적인- 환경 상태가 되었는지)을 기초로 하여 보상을 최대화 하는 일련의 단계를 학습한다.

#### 1.2.3 비지도 학습으로 숨겨진 구조 발견

- 군집 : 사전 정보 없이 쌓여있는 데이터를 클러스터로 조직하는 탐색적 데이터 분석 기법이다. 예를 들면 레이블되지 않은 데이터를 특성 x1과 x2의 유사도를 기반으로 세 개의 개별적인 그룹으로 조직화한다.
  ![클러스터링](image/클러스터링.jpg)

- 차원 축소 : 관련 있는 정보를 대부분 유지하면서 더 작은 차원의 부분 공간(입력 데이터의 특성을 하나의 축으로 생각한 벡터 공간에서 특성을 줄여 차원이 낮아진 데이터는 부분 공간이라고 표현)으로 데이터를 압축한다. 데이터 시각화에도 유용하다. 예를 들면 고차원 특성을 1차원 또는 2차원, 3차원 특성 공간으로 투영하여 3D와 2D 산점도나 히스토그램으로 시각화한다.
  ![차원축소](image/차원축소.jpg)

### 1.4 머신 러닝 시스템 구축 로드맵

![머신러닝로드맵](image/머신러닝로드맵.jpg)

#### 1.4.1 전처리: 데이터 형태 갖추기

- 특성 추출 : 원본 데이터는 일련의 꽃 이미지들인데 여기서 꽃의 색깔, 높이, 꽃의 길이와 너비 등을 추출해내는 것
- 스케일(scale) 조정 : 많은 머신 러닝 알고리즘에서 최적의 성능을 내려면 선택된 특성이 같은 스케일을 가져야 한다. 특성을 [0, 1] 범위로 변환하거나 평균이 0이고 단위 분산을 가진 표준 정규 분포로 변환하는 경우가 많다.
- 특성 선택
- 차원 축소 : 일부 특성 간의 상관관계가 높아 중복된 정보를 가진 경우, 차원 축소 기법을 사용하여 특성을 저차원 부분 공간으로 압축한다. 필요한 저장 공간을 줄이고 학습 속도를 높일 수 있다. 데이터셋에 관련 없는 특성(잡음)이 매우 많은 경우-잡음 대비 신호의 크기가 낮은 경우-에는 차원 축소가 모델의 예측 성능을 높이기도 한다.
- 샘플링

#### 1.4.2 예측 모델 훈련과 선택

- 성능 지표 및 모델 선택 : 가장 좋은 모델을 선택하기 위해서는 성능을 측정할 지표-accuracy 등-를 결정한 뒤 최소한 몇 가지 알고리즘을 비교해야 한다.
- 교차 검증 : 데이터셋을 train data와 test data로 나누어 모델을 학습하는데, test data에 적용하여 평가하기 전 train data를 다양한 교차 검증 기법을 사용하여 평가한다.
- 하이퍼파라미터 최적화 : 데이터에서 학습하는 파라미터가 아니라 모델 선응을 향상하기 위해 사용하는 다이얼로 생각할 수 있다.

#### 1.4.3 모델을 평가하고 본 적 없는 샘플로 예측

- 특성 스케일 조정과 차원 축소와 같은 단계에서 사용한 파라미터는 훈련 세트만 사용하여 얻은 것이며, 동일한 파라미터를 테스트 세트는 물론 새로운 모든 샘플을 변환하는데 사용해야한다. 만약 테스트 세트의 특성을 반영한 새로운 파라미터를 사용하여 테스트 세트를 변환하면 과도하게 낙관적인 결과가 나온다.

## 2장 간단한 분류 알고리즘 훈련

- 퍼셉트론
- 적응형 선형 뉴런

### 2.1 인공 뉴런: 초기 머신 러닝의 간단한 역사

- 맥컬록-피츠(MCP) 뉴런은 신경 세포를 이진 출력을 내는 간단한 논리 회로로 표현했다. 수상 돌기에 여러 신호가 도착하면 세포체에 합쳐지고, 합쳐진 신호가 특정 임계 값을 넘으면 출력 신호가 생성되어 축삭 돌기를 이용하여 전달된다.
- 로젠블라트는 MCP 뉴런 모델을 기반으로 퍼셉트론 학습 개념을 발표했다. 퍼셉트론 규칙에서 자동으로 최적의 가중치-뉴런의 출력 신호를 낼지 말지를 결정하기 위해 입력 특성에 곱하는 계수-를 학습하는 알고리즘을 제안했다. 이 알고리즘으로 샘플이 한 클래스에 속하는지 아닌지를 예측할 수 있다.

#### 2.1.1 인공 뉴런의 수학적 정의

- 인공 뉴런 아이디어는 두 개의 클래스가 있는 이진 분류 작업으로 볼 수 있다. 결정 함수 $\phi$(z)는 입력 벡터 x와 이에 상응하는 가중치 벡터 w의 선형 조합으로 계산되는 최종 입력 z에 변형된 단위 계단 함수(unit step function) $\phi$( )를 씌운 함수이다.
  - z = $w_1 x_1 + ... + w_m x_m$
  - $\phi$( ) = z >= $\theta$일 때, 1, 그렇지 않으면 -1을 반환
- 특정 i번째 샘플 $x^{(i)}$에 대해 계산된 z값이 사전에 정의된 임계 값 $\theta$보다 크면 클래스 1로 예측하고, 그렇지 않으면 클래스 -1로 예측한다. $\theta$를 식의 왼쪽-z식-으로 넘겨 이를 절편으로 표현함으로써 z를 조금 더 간단하게 쓸 수 있다. 의미상으로는 임계 값을 0으로 변경하는 대신 $w_0=- \theta$이고 $x_0=1$인 0번째 가중치를 추가하는 것이 된다.
- 왼쪽 이미지는 퍼셉트론 결정 함수로 최종 입력 z = $w^Tx$가 이진 출력(-1 또는 1)으로 압축되는 방법이다. 오른쪽 이미지는 이를 사용하여 선형 분리가 가능한 두 개의 클래스 사이를 구별하는 방법이다.

![퍼셉트론](image/퍼셉트론.jpg)

#### 2.1.2 퍼셉트론 학습 규칙

- MCP 뉴런과 로젠블라트의 임계 퍼셉트론 모델은 출력을 내거나 내지 않는 두 가지 경우만 있다. 로젠블라트의 초기 퍼셉트론 학습 규칙은 다음과 같다.

  1. 가중치를 0 또는 랜덤한 작은 값으로 초기화(세팅) 한다.
  2. 각 훈련 샘플 $x^{(i)}$에서 다음 작업을 한다.

  - a. 출력 값 $\hat{y}$를 계산한다.
  - b. 가중치를 업데이트한다.

- 가중치 벡터 w에 있는 개별 가중치 $w_j$는 동시에 업데이트 된다.

  - $w_j := w_j + \Delta w_j$

- $\Delta w_j = \eta \bigl(y^{(i)}-\hat{y}^{(i)}\bigr) x^{(i)}_j$

  - $\Delta w_j$ : $i$번째 train 샘플($i$번째 행)의 입력 특성 중 $j$번째 특성의 가중치이다.
  - $\eta$ : 학습률. 일반적으로 0.0에서 1.0 사이의 값이다. 모든 가중치 업데이트에 $(y^{(i)}-\hat{y}^{(i)})$의 값을 얼만큼 반영할지 조절한다.
  - $(y^{(i)}-\hat{y}^{(i)})$ : $y^{(i)}$가 1인데 -1로 예측했다면 2가 되어 가중치 업데이트의 방향을 결정한다. 만약 퍼셉트론이 클래스 레이블을 정확히 예측했다면 이 값은 0이 되어 가중치 업데이트가 발생하지 않는다.
    - $y^{(i)}$ : $i$번째 train 샘플($i$번째 행)의 진짜 클래스 레이블. 1또는 -1
    - $\hat y^{(i)}$ : $i$번째 train 샘플($i$번째 행)의 예측 클래스 레이블. 1또는 -1
  - $x^{(i)}_j$ : $i$번째 train 샘플($i$번째 행)의 입력 특성 중 $j$번째 특성이다. $x^{(i)}_j$의 크기가 크면 해당 특성의 가중치 업데이트 값을 비례하게 크게 만들어준다. 만약 $\hat y^{(i)}$을 잘못 예측하여 반대 방향으로 가중치를 업데이트 하는 경우, $x^{(i)}_j$의 값이 큰데 $w^{(i)}_j$가 충분히 크지 않다면 z( = $w_1 x_1 + ... + w_m x_m$)가 임계치를 넘어 제대로 예측될 확률이 작다.

- 퍼셉트론은 두 클래스가 선형적으로 구분되고 학습률이 충분히 작을 때만 수렴이 보장된다. 두 클래스를 선형 결정 경계로 나눌 수 없다면 훈련 데이터셋을 반복할 최대 횟수(에포크(epoch))를 지정하고 분류 허용 오차를 지정하여 가중치 업데이트를 끝낼 수 있다.

![선형구분_데이터셋](image/p053_선형구분_데이터셋.jpg)
